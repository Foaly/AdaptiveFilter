\section{Einführung}
\label{sec:intor}

Die vorliegende Aufgabe dreht sich um die Systemidentifikation zweier System.
Dafür wurden in der Aufgabenstellung jeweils ein FIR und ein IIR Filter gegeben.
Mit Hilfe der in der Vorlesung besprochenen adaptiven Filter sollten die Koeffizienten der gegebenen System identifiziert werden.
Als Vorbereitung auf die Systemidentifikation wurden der Least-Mean-Square Algorithmus (LMS) und der Recursive-Least-Squares Algorithmus mit Vergessenfaktor (RLS) implementiert.
Die Implementation erfolgte in Python\footnote{Software Repository: \url{https://github.com/Foaly/AdaptiveFilter}} anhand der Rechenvorschrift nach Moschytz \cite{moschytz2000book} (LMS nach S.85 und RLS nach S.145).

\section{FIR-Filter}
\label{sec:fir}

Die erste Aufgabe ist die Systemidentifikation eines FIR Filters mit 5 Koeffizienten.
Das System wird mit Hilfe der LMS und RLS (mit Vergessensfaktor) Algorithmen identifiziert.
Der Einfluss unterschiedlicher Parameter, wie die Anzahl der Filterkoeffizienten, die Varianz des hinzugefügten Rauschens und die Werte für Schrittweite oder Vergessensfaktor auf das Ergebnis wird dabei untersucht.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{figures/FIR_System.png}
 \caption{Blockschaltbild des zu untersuchenden FIR Systems (${h_0 = 0.7,}$ ${h_1 = 0.1,}$ ${h_2 = -0.03,}$ ${h_3 = 0.18,}$ ${h_4 = -0.24}$) \cite{aufgabenstellung}.}
	\label{fig:FIR_System}
\end{figure}

In den entstandenen Plots werden die Ergebnisse des LMS und KLS Algorithmus mit Hilfe von je zwei Plots charakterisiert. 
Auf der linken Seite ist der quadratischen Fehlers (Mean-Square-Error bzw. MSE) über den zeitlichen Verlauf in Samplen dargestellt. 
Ein gegen 0 konvergierender Verlauf des Fehlers ist wünschenswert, da dies bedeutet, dass die Differenz zwischen Ausgang des Filters und dem gewünschten Signal kleiner wird und der Algorithmus sein Ergebnis damit selbst verbessert.
Die gestrichelte rote Linie zeigt den Durchschnittswert des Fehlers an, welche es vereinfacht den Wert abzulesen, auf den der Fehler konvergiert.
Außerdem sei angemerkt, dass der Plot des Fehler mit einem Rolling Average von 30 Samplen geglättet wurde, um einzelne Ausreißer zu entfernen und die Lesbarkeit zu erhöhen.
Auf der rechten Seite der Abbildung ist eine Verlaufskurve der des Algorithmus gelernte Filterkoeffizienten zu sehen, woraus sich das Adaptionsverhalten des Algorithmus ablesen lässt.
Im Idealfall sollten auch diese je auf einen Wert konvergieren.
Ist dies nicht der Fall, so zeigt dies, dass das Eingangssignal ungeeignet für die Bearbeitung mit diesem Algorithmus ist oder die Einstellung des Algorithmus nicht optimal sind.



\subsection{Anzahl an Filterkoeffizienten}

Die Anzahl der Filterkoeffizienten ist einer der Parameter, der die Systemidentifikation am Stärksten beeinflusst. 
Zu erwarten wäre, dass sich das zu identifizierende System mit weniger Koeffizienten als nötig nicht hinreichend genau abbilden lässt. 
Nach dem Testen verschiedener Mengen an Filterkoeffizienten ($N \in \{1, 2, 5\}$) erwies sich diese Annahme auch als korrekt.
Dabei wird deutlich, dass sowohl dem LMS als auch dem RLS Algorithmus die Approximation mit zu wenig Filterkoeffizienten gleich gut gelingt.
Vergleicht man die Fluktuation in der Fehlerkurve in Abbildungen \ref{fig:N1} und \ref{fig:N2} so wird deutlich, dass diese bei beide Algorithmen nahezu identisch ist.
Lediglich der Anfang des LMS Algorithmus weißt einen höheren Fehler auf, welcher auf Grund der längeren Konvergierungszeit von der Glättung der Plots nicht so stark beeinflusst wird.
Betrachtet man jedoch den Verlauf der Koeffizient über die Laufzeit der Algorithmen, so fällt auf, dass der LMS länger benötigt bis die Koeffizienten auf einen Wert konvergieren.
Der Koeffizientenverlauf des RLS hat nach Erreichen des Wertes auch ein deutlich stabileres und rauschärmeres Verhalten als die Koeffizienten des LMS.
Dies ist jedoch keine Eingenschaft der Algorithmen als solche, sonder wird durch die Einstellung der Parameter des Vergessensfaktors $\rho$ des RLS und der Schrittweite $\mu$ des LMS bedingt (mehr in Abschnitt \ref{sec:FIR_rho} und \ref{sec:FIR_mu}).
Die unterschiedliche Geschwindigkeit mit der die beiden Algorithmen konvergieren ist ebenfalls darauf zurück zu führen.
Die Legende in Abbildung \ref{fig:N5} zeigt deutlich, dass beide Algorithmen mit der korrekten Anzahl an Koeffizienten auf das identische, korrekte Ergebnis kommen (vgl. mit Abb. \ref{fig:FIR_System}).

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{{{figures/FIR/N_1_var_0.001}}}
 \caption{Vergleich zwischen LMS und RLS mit je einem Filterkoeffizienten (N=1)}
	\label{fig:N1}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{{{figures/FIR/N_2_var_0.001}}}
 \caption{Vergleich zwischen LMS und RLS mit je zwei Filterkoeffizienten (N=2)}
	\label{fig:N2}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{{{figures/FIR/N_5_var_0.001}}}
 \caption{Vergleich zwischen LMS und RLS mit je fünf Filterkoeffizienten (N=5)}
	\label{fig:N5}
\end{figure}

\subsection{Einfluss der Rauschvarianz}
In der Messstrecke des Systems wird ein AWGN (Additive White Gaussian Noise bzw. Additives Weißes Gausverteiltes Rauschen) hinzugefügt.
Dies ist als $n[k]$ in der schematischen Darstellung A.2 in \cite{moschytz2000book} (S. 211) der Messstrecke gekennzeichnet. 
Dadurch lässt sich Testen, welche Algorithmus stabiler und weniger anfällig gegenüber Rauschen ist und sich besser für eine schlechte Übertragungsstrecke einsetzen lässt.
Für die Varianz des Rauschens $\sigma^2$ wurden verschiedene Werte angenommen ($\sigma^2 \in \{0.001, 0.1, 1, 10\}$) und ihr Einfluss auf das Fehlerverhalten der adaptiven Algorithmen untersucht.
Wenig überraschend bedeutet eine Zunahme der Varianz des Rauschen ebenfalls eine Zunahme im quadratischen Fehler und im Adaptionsverhalten der Filterkoeffizienten.
Vergleicht man die Abbildung \ref{fig:N5}, welche mit einem AWGN mit $\sigma^2 = 0.001$ berechnet wurde, mit der Abbildung \ref{fig:sig0.1} ($\sigma^2 = 0.1$) so wird deutlich wie stark das Rauschen die Leistung des adaptiven Filters schwächt.
In Abbildung \ref{fig:N5}, \ref{fig:sig0.1}, \ref{fig:sig1} und \ref{fig:sig10} ist außerdem erkennbar, dass die Filter mit ansteigenden Werten für $\sigma^2$ auf einen jeweils angestiegenen Fehlerwert konvergieren.
In \ref{fig:sig10} mit $\sigma^2 = 10$ ist im Plot des quadratischen Fehlers kein Konvergieren mehr sichtbar.
Bei Betrachten des Verlaufs der Koeffizienten mit verschiedenen Werten für $\sigma^2$ fällt erneut auf, dass der RLS im Vergleich zum LMS eine stärke Toleranz gegenüber Rauscheinflüssen aufweist.
Ebenfalls auffällig ist die deutliche Ähnlichkeit der beiden Fehlerkurven der unterschiedlichen Algorithmen in Abbildung \ref{fig:sig10}.
Dies lässt darauf schließen, dass der Signal-Rausch-Abstand (SNR) so gering ist, dass das Rauschen das eigentlich Signal nahezu komplett überdeckt.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{{{figures/FIR/N_5_var_0.1}}}
 \caption{Vergleich zwischen LMS und RLS bei einem AWGN von $\sigma^2 = 0.1$  (N=5)}
	\label{fig:sig0.1}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{{{figures/FIR/N_5_var_1.0}}}
 \caption{Vergleich zwischen LMS und RLS bei einem AWGN von $\sigma^2 = 1$  (N=5)}
	\label{fig:sig1}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{{{figures/FIR/N_5_var_10.0}}}
 \caption{Vergleich zwischen LMS und RLS bei einem AWGN von $\sigma^2 = 10$  (N=5)}
	\label{fig:sig10}
\end{figure}


\subsection{Einfluss des Vergessensfaktors \boldmath{$\rho$}}
\label{sec:FIR_rho}

Der Vergessensfaktor $\rho$ ist ein Parameter des RLS Algorithmus und liegt im Wertebereich $0 < \rho \leq 1$.
Er dient als Faktor, der bestimmt wie stark in der Vergangenheit zurückliegende Werte gewichtet werden.
Ein Vegessensfaktor von 1 bedeutet, dass alle bisherigen Wert in die Berechnung mit einbezogen werden.
Ist der Wert von $\rho$ sehr gering, so werden nur die kleiner Teil der am Kürzesten zurückliegenden Werte berücksichtigt.
Während ein hoher Wert dazu führt, dass der RLS Algorithmus sich schlecht an ein sich veränderndes System anpassen kann (siehe Abschnitt \ref{sec:Systemwechsel} Systemwechsel), führt ein zu niedriger Wert zu Stabilitätsproblemen, da nie genug Werte miteinbezogen werden, um das System zu identifizieren.
Betrachtet man die Abbildungen \ref{fig:rho_vergleich} so wird dies ebenfalls deutlich.
Ein niedrigerer Wert für $\rho$ führt zu einem höheren quadratischen Fehler und einem deutlich sprunghaftem und rauschhaften Adaptionsverhalten der Filterkoeffizienten.
Laut Moschytz \cite{moschytz2000book} (S. 145) liegt der Wert für den Vergessensfaktor typischerweise zwischen $0.95 < \rho < 1$.
Bei den bisher abgebildeten Plots \ref{fig:N1} bis \ref{fig:sig10} wurde jeweils ein Vergessensfaktor $\rho = 0.99$ verwendet.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{{{figures/FIR/N_2_var_0.001_rho_vergleich}}}
 \caption{Vergleich zwischen verschiedenen Werten für $\rho$ des RLS ($\rho = 0.5$ oben und $\rho = 0.99$ unten) }
	\label{fig:rho_vergleich}
\end{figure}



\subsection{Einfluss des Schrittweite \boldmath{$\mu$}}
\label{sec:FIR_mu}

Die Schrittweite $\mu$ ist ein Parameter des LMS Algorithmus.
Er kann Werte zwischen $0 < \mu \leq 1$ annehmen und ist ein Faktor der bei jeder Iteration den Fehler $e[n] = d[n] - y[n]$ gewichtet.
Dadurch lässt sich die Geschwindigkeit der Adaption des Algorithmus einstellen.
Ein hoher Wert lässt den Fehler schneller konvergieren, kann allerdings dazu führen, dass das optimal Fehlerminimum nicht erreicht werden kann und dieses in jeder Iteration über- bzw. unterschritten wird.
In den Abbildungen \ref{fig:mu0.01} bis \ref{fig:mu0.2} ist dies mit Hilfe verschiedener Werte für $\mu$ beispielhaft dargestellt ($N=5, \sigma^2=0.01$).
In Abbildung \ref{fig:mu0.01} ist $\mu = 0.001$ was zu einer vergleichsweise langsamen An­nä­herung des Fehlers an den Wert 0 führt, welcher aber erreicht wird.
Wird, wie in Abbildung \ref{fig:mu0.1}, $\mu = 0.1$ gesetzt konvergiert der Fehler zwar schneller, aber erreicht 0 nie ganz (siehe rote gestrichelte Linie für den Durchschnitt) und oszilliert leicht um diesen Wert.
Ein noch höherer Wert, wie $\mu = 0.2$ verdeutlicht das Über- und Unterschreiten des optimalen Fehlerminimums deutlich (siehe \ref{fig:mu0.2})
Die bisherigen Plots \ref{fig:N1} bis \ref{fig:sig10} wurden mit einer Schrittweite von $\mu = 0.01$ erzeugt.


\begin{figure}[H]
  \centering
      \includegraphics[width=0.7\textwidth]{{{figures/FIR/lms_N_5_var_0.001_mu_0.01}}}
 \caption{LMS mit einer Schrittweite $\mu = 0.01$}
	\label{fig:mu0.01}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.7\textwidth]{{{figures/FIR/lms_N_5_var_0.001_mu_0.1}}}
 \caption{LMS mit einer Schrittweite $\mu = 0.1$}
	\label{fig:mu0.1}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.7\textwidth]{{{figures/FIR/lms_N_5_var_0.001_mu_0.2}}}
 \caption{LMS mit einer Schrittweite $\mu = 0.2$}
	\label{fig:mu0.2}
\end{figure}



\subsection{Wahl des Lernalgorithmus}

generell: 
- unterschiedliche Konvergenzeit
- unterschiedliche Laufzeit (RLS mehr)
- RLS ist schneller als LMS, weniger Fluktuation
- RLS reagiert schneller
- RLS leicht weniger Rausch empfindlich
- bei unveränderlichen Systemen sehr ähnliche ergebnisse



\files{main.py}


\section{IIR-Filter}
\label{sec:iir}

Das zweite System das untersucht werden soll ist ein IIR Filter.

Hierbei ist zu beachten, dass es unter idealen Bedingungen (kein Rauschen, korrekte Parameter etc.) möglich ist ein FIR Filter mithilfe des LMS und KLS Algorithmus perfekt nachzubilden.
Bei einem IIR System ist dies nicht exakt möglich, da es eine rekursiven Anteil besitzt, welcher in den LMS und KLS Algorithmen nicht abgebildet wird.
Auf Grundlage dessen ist nur ein 

\begin{figure}[H]
  \centering
      \includegraphics[width=0.5\textwidth]{figures/IIR_System.png}
 \caption{Blockschaltbild des zu untersuchenden IIR Systems ($h_1 = 0.82, h_2 = -0.03$).}
	\label{fig:FIR_System}
\end{figure}



\subsection{Anzahl an Filterkoeffizienten}
\subsection{Wahl des Lernalgorithmus}
\subsection{Einfluss der Varianz}


\section{Systemwechsel}
\label{sec:Systemwechsel}

\subsection{Adaptionsverhalten}


\section{Kernel Least Mean Squares}
\label{sec:4}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.7\textwidth]{{{figures/klms_5}}}
  \caption{Training mit KLMS und 5 Filterkoeffizienten}
  \label{fig:klms5}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=0.7\textwidth]{{{figures/klms_10}}}
  \caption{Training mit KLMS 10 Filterkoeffizienten}
  \label{fig:klms5}
\end{figure}

\section{Fehlerfunktion(en)}
